{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f8201b1-155b-47a3-843f-d5a1f25fcbe1",
   "metadata": {},
   "source": [
    "# Climate Risk Premium Analysis: Data Cleaning & Feature Engineering\n",
    "\n",
    "**Project**: Analyzing Climate Risk Premiums in US Equity Markets  \n",
    "**Notebook**: 2. Data Cleaning & Feature Engineering  \n",
    "**Author**: Anush Nepal\n",
    "\n",
    "## Objective\n",
    "Transform raw stock price data into analysis-ready features for climate risk analysis:\n",
    "- Calculate **daily returns** and **volatility measures**\n",
    "- Create **event windows** around climate events\n",
    "- Generate **sector performance metrics**\n",
    "- Prepare data for **event study analysis**\n",
    "\n",
    "## Data Overview\n",
    "Starting with **90,495 records** from **45 companies** across 3 climate-sensitive sectors (2017-2024)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e084ee-242b-4597-ba60-f993cd2f102d",
   "metadata": {},
   "source": [
    "## 1. Importing Libraries & Loading Data\n",
    "Loading cleaned dataset and setting up the analysis environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffe76ab2-3699-489b-8372-568edea18a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully.\n",
      "Analysis date: 2025-08-14\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "print(\"Libraries imported successfully.\")\n",
    "print(f\"Analysis date: {datetime.now().strftime('%Y-%m-%d')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d719f4d6-bdcd-45a4-ba5f-e5f950075f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "- Stock data loaded : (90495, 10)\n",
      "- Climate events loaded: (7, 4)\n",
      "- Date range: 2017-01-03 to 2024-12-30\n",
      "- Companies: 45\n",
      "- Sectors: Energy, Insurance, Real Estate\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading dataset...\")\n",
    "df = pd.read_csv('../data/raw/complete_stock_data.csv') # Loading main stock data\n",
    "events_df = pd.read_csv('../data/raw/climate_events.csv') # Loading climate events\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "events_df['Date'] = pd.to_datetime(events_df['Date'])\n",
    "print(f\"- Stock data loaded : {df.shape}\")\n",
    "print(f\"- Climate events loaded: {events_df.shape}\")\n",
    "print(f\"- Date range: {df['Date'].min().date()} to {df['Date'].max().date()}\")\n",
    "print(f\"- Companies: {df['Ticker'].nunique()}\")\n",
    "print(f\"- Sectors: {', '.join(df['Sector'].unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b857cc-81d2-4cdb-8ab2-488f3187a9fa",
   "metadata": {},
   "source": [
    "## Data Preparation & Cleaning\n",
    "Ensuring the data is properly formatted and handling any edge cases, before analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57f7cc85-690f-46c2-b645-e14fd71082a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data for analysis...\n",
      "Converting date columns...\n",
      "- Date conversion successful\n",
      "- df['Date'] dtype: datetime64[ns]\n",
      "- events_df['Date'] dtype: datetime64[ns]\n",
      "\n",
      "Date Integrity Check:\n",
      "- Records per company: count     45.0000\n",
      "mean    2011.0000\n",
      "std        0.0000\n",
      "min     2011.0000\n",
      "25%     2011.0000\n",
      "50%     2011.0000\n",
      "75%     2011.0000\n",
      "max     2011.0000\n",
      "dtype: float64\n",
      "- Price columns, any negatives? False\n",
      "- Volume, any negatives? False\n",
      "- Price consistency check: Failed\n",
      "\n",
      "Sample of cleaned data:\n",
      "                 Date Ticker     Sector    Open   Close  Volume\n",
      "0 2017-01-03 05:00:00    AFG  Insurance 45.1676 44.8207  210100\n",
      "1 2017-01-04 05:00:00    AFG  Insurance 44.9738 45.2901  339500\n",
      "2 2017-01-05 05:00:00    AFG  Insurance 45.2748 44.8105  212400\n",
      "3 2017-01-06 05:00:00    AFG  Insurance 44.7901 44.8105  152700\n",
      "4 2017-01-09 05:00:00    AFG  Insurance 44.8514 44.2545  157800\n",
      "5 2017-01-10 05:00:00    AFG  Insurance 44.3973 44.4484  185700\n",
      "6 2017-01-11 05:00:00    AFG  Insurance 44.4857 44.9158  297900\n",
      "7 2017-01-12 05:00:00    AFG  Insurance 44.8595 44.5369  160000\n",
      "8 2017-01-13 05:00:00    AFG  Insurance 44.6240 44.7417  114300\n",
      "9 2017-01-17 05:00:00    AFG  Insurance 44.3219 44.1888  239800\n"
     ]
    }
   ],
   "source": [
    "print(\"Preparing data for analysis...\")\n",
    "df = df.sort_values(['Ticker', 'Date']).reset_index(drop=True)\n",
    "print(\"Converting date columns...\") # Converting Date column from text to proper datetime\n",
    "df['Date'] = pd.to_datetime(df['Date'], utc=True).dt.tz_localize(None)\n",
    "events_df['Date'] = pd.to_datetime(events_df['Date'])\n",
    "\n",
    "# Removing timezone from events if it exists\n",
    "if hasattr(events_df['Date'].dtype, 'tz') and events_df['Date'].dt.tz is not None:\n",
    "    events_df['Date'] = events_df['Date'].dt.tz_localize(None)\n",
    "print(f\"- Date conversion successful\")\n",
    "print(f\"- df['Date'] dtype: {df['Date'].dtype}\")\n",
    "print(f\"- events_df['Date'] dtype: {events_df['Date'].dtype}\")\n",
    "\n",
    "print(\"\\nDate Integrity Check:\")\n",
    "records_per_company = df.groupby('Ticker').size()\n",
    "print(f\"- Records per company: {records_per_company.describe().round(0)}\")\n",
    "print(f\"- Price columns, any negatives? {(df[['Open', 'High', 'Low', 'Close']] < 0).any().any()}\")\n",
    "print(f\"- Volume, any negatives? {(df['Volume'] < 0).any()}\")\n",
    "\n",
    "price_check = (\n",
    "    (df['High'] >= df['Low']) &\n",
    "    (df['Close'] <= df['High']) &\n",
    "    (df['Close'] >= df['Low'])\n",
    ").all()\n",
    "print(f\"- Price consistency check: {'Passed' if price_check else 'Failed'}\")\n",
    "\n",
    "print(\"\\nSample of cleaned data:\")\n",
    "display_cols = ['Date', 'Ticker', 'Sector', 'Open', 'Close', 'Volume']\n",
    "print(df[display_cols].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe1d2bf7-c37f-424a-8f90-6df2979d2f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Investigating price consistency...\n",
      "Records with price issues: 26\n",
      "\n",
      "First few problematic records:\n",
      "                    Date Ticker     Open     High      Low    Close\n",
      "269  2018-01-29 05:00:00    AFG  61.4468  61.8387  61.1086  61.1086\n",
      "627  2019-07-02 04:00:00    AFG  59.0735  59.3305  58.7022  59.3305\n",
      "694  2019-10-07 04:00:00    AFG  59.6032  59.6204  59.0872  59.0872\n",
      "794  2020-03-02 05:00:00    AFG  54.8342  57.2561  54.2933  57.2561\n",
      "8568 2019-02-04 05:00:00    ARE 103.6642 104.1095 102.8532 104.1095\n",
      "\n",
      "Types of issues:\n",
      "- High < Low: 0 records\n",
      "- Close > High: 15 records\n",
      "- Close < Low: 11 records\n",
      "\n",
      "Total problematic records: 26 out of 90495 (0.0287%)\n",
      "- Very small number of issues--safe to remove these records\n",
      "- Cleaned dataset: 90469 records (removed 26)\n",
      "- Price consistency check after cleaning: Passed\n",
      "\n",
      "Final dataset size: 90,469 records\n"
     ]
    }
   ],
   "source": [
    "print(\"Investigating price consistency...\")\n",
    "price_issues = df[\n",
    "    (df['High'] < df['Low']) |  # High should be >= Low\n",
    "    (df['Close'] > df['High']) |  # Close should be <= High  \n",
    "    (df['Close'] < df['Low'])     # Close should be >= Low\n",
    "]\n",
    "print(f\"Records with price issues: {len(price_issues)}\")\n",
    "\n",
    "if len(price_issues) > 0:\n",
    "    print(\"\\nFirst few problematic records:\")\n",
    "    print(price_issues[['Date', 'Ticker', 'Open', 'High', 'Low', 'Close']].head())\n",
    "    high_low_issues = len(df[df['High'] < df['Low']]) # Checking type of issues\n",
    "    close_high_issues = len(df[df['Close'] > df['High']])\n",
    "    close_low_issues = len(df[df['Close'] < df['Low']])\n",
    "    print(f\"\\nTypes of issues:\")\n",
    "    print(f\"- High < Low: {high_low_issues} records\")\n",
    "    print(f\"- Close > High: {close_high_issues} records\") \n",
    "    print(f\"- Close < Low: {close_low_issues} records\")\n",
    "    print(f\"\\nTotal problematic records: {len(price_issues)} out of {len(df)} ({len(price_issues)/len(df)*100:.4f}%)\")\n",
    "    \n",
    "    if len(price_issues) < 100:  # If very few issues\n",
    "        print(\"- Very small number of issues--safe to remove these records\")\n",
    "        df_clean = df[~df.index.isin(price_issues.index)].copy()\n",
    "        print(f\"- Cleaned dataset: {len(df_clean)} records (removed {len(price_issues)})\")\n",
    "        df = df_clean # Updating  main dataframe\n",
    "        price_check_new = ( # Rechecking\n",
    "            (df['High'] >= df['Low']) & \n",
    "            (df['Close'] <= df['High']) & \n",
    "            (df['Close'] >= df['Low'])\n",
    "        ).all()\n",
    "        print(f\"- Price consistency check after cleaning: {'Passed' if price_check_new else 'Still Failed'}\")\n",
    "    \n",
    "else:\n",
    "    print(\"No price issues found.\")\n",
    "\n",
    "print(f\"\\nFinal dataset size: {len(df):,} records\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
