{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be98651b-1882-4d94-981a-062e5bcdd92a",
   "metadata": {},
   "source": [
    "# Climate Risk Premium Analysis: Data Collection\n",
    "\n",
    "**Project**: Analyzing Climate Risk Premiums in US Equity Markets  \n",
    "**Notebook**: 1. Data Collection  \n",
    "**Author**: Anush Nepal  \n",
    "\n",
    "## Objective\n",
    "This notebook collects stock price data for companies in three climate-sensitive sectors:\n",
    "- **Energy**: Direct exposure to transition risks and carbon regulations\n",
    "- **Insurance**: Companies that price climate risks into their business models\n",
    "- **Real Estate**: Physical assets exposed to climate events\n",
    "\n",
    "Gathering 7 years of data (2017-2024) to analyze how climate events impact stock prices and whether investors price climate risk into valuations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a817a6c-ae3f-4187-877e-8e0ce5ce413f",
   "metadata": {},
   "source": [
    "## 1. Importing Required Libraries\n",
    "Following are the Python libraries we'll use for data collection and analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aaeb9afb-d746-4689-8078-12ebf9f2e36e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully.\n",
      "Pandas version: 2.2.3\n",
      "Current date: 2025-08-13\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf # For stock data collection\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import os \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(\"Libraries imported successfully.\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"Current date: {datetime.now().strftime('%Y-%m-%d')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d20cf3e-a491-4588-89e3-d4d089f3293d",
   "metadata": {},
   "source": [
    "## 2. Defining Company Lists by Sector\n",
    "We'll focus on large-cap companies in each sector to ensure data quality and liquidity. These firms represent the major players that institutional investors would focus on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2c51227-94d5-4756-b0a5-58fb3199493f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total companies selected: 45\n",
      "Energy: 15 companies\n",
      "- Sample tickers: XOM, CVX, COP, EOG, SLB...\n",
      "Insurance: 15 companies\n",
      "- Sample tickers: BRK-B, PGR, TRV, ALL, CB...\n",
      "Real Estate: 15 companies\n",
      "- Sample tickers: PLD, AMT, CCI, EQIX, WELL...\n"
     ]
    }
   ],
   "source": [
    "# Energy Sector (major oil, gas, and renewable energy companies)\n",
    "energy_tickers = [\n",
    "    'XOM', # Exxon Mobil\n",
    "    'CVX', # Chevron\n",
    "    'COP', # ConocoPhillips\n",
    "    'EOG', #EOG Resources\n",
    "    'SLB', # Schlumberger\n",
    "    'PXD', # Pioneer Natural Resources\n",
    "    'KMI', # Kinder Morgan\n",
    "    'OXY', # Occidental Petroleum\n",
    "    'VLO', # Valero Energy\n",
    "    'PSX', # Phillips 66\n",
    "    'MPC', # Marathon Petroleum\n",
    "    'HAL', # Halliburton\n",
    "    'BKR', # Baker Hughes\n",
    "    'DVN', # Devon Energy\n",
    "    'FANG' # Diamondback Energy\n",
    "]\n",
    "\n",
    "# Insurance Sector (property & casualty insurers most exposed to climate risk)\n",
    "insurance_tickers = [\n",
    "    'BRK-B', # Berkshire Hathaway\n",
    "    'PGR',   # Progressive\n",
    "    'TRV',   # Travelers\n",
    "    'ALL',   # Allstate\n",
    "    'CB',    # Chubb\n",
    "    'AIG',   # American International Group\n",
    "    'HIG',   # Hartford Financial\n",
    "    'CNA',   # CNA Financial\n",
    "    'RLI',   # RLI Corp\n",
    "    'AFG',   # American Financial Group\n",
    "    'CINF',  # Cincinnati Financial\n",
    "    'WRB',   # W.R. Berkley\n",
    "    'Y',     # Alleghany\n",
    "    'EG',    # Everest Group\n",
    "    'KMPR'   # Kemper\n",
    "]\n",
    "\n",
    "# Real Estate Sector (REITs and real estate companies)\n",
    "real_estate_tickers = [\n",
    "    'PLD',   # Prologis (logistics real estate)\n",
    "    'AMT',   # American Tower (cell towers)\n",
    "    'CCI',   # Crown Castle (cell towers)\n",
    "    'EQIX',  # Equinix (data centers)\n",
    "    'WELL',  # Welltower (healthcare real estate)\n",
    "    'DLR',   # Digital Realty Trust\n",
    "    'SPG',   # Simon Property Group (malls)\n",
    "    'O',     # Realty Income\n",
    "    'AVTR',  # Avantax (residential)\n",
    "    'EXR',   # Extended Stay America\n",
    "    'AVB',   # AvalonBay Communities\n",
    "    'EQR',   # Equity Residential\n",
    "    'MAA',   # Mid-America Apartment Communities\n",
    "    'ESS',   # Essex Property Trust\n",
    "    'UDR'    # UDR Inc\n",
    "]\n",
    "\n",
    "# Combining all sectors into a master dictionary\n",
    "sector_tickers = {\n",
    "    'Energy': energy_tickers,\n",
    "    'Insurance': insurance_tickers,\n",
    "    'Real Estate': real_estate_tickers\n",
    "}\n",
    "\n",
    "# Summary\n",
    "total_companies = sum(len(tickers) for tickers in sector_tickers.values())\n",
    "print(f\"Total companies selected: {total_companies}\")\n",
    "for sector, tickers in sector_tickers.items():\n",
    "    print(f\"{sector}: {len(tickers)} companies\")\n",
    "    print(f\"- Sample tickers: {', '.join(tickers[:5])}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f95482-06c7-4ceb-a1b6-85bb6019168b",
   "metadata": {},
   "source": [
    "## 3. Defining Climate Events for Analysis\n",
    "We'll focus on major climate events from 2017-2024 that had significant economic impact and media coverage. These events provide natural experiments to study market reactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5c618cb-1d0f-408b-9b47-341555a3bd75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Climate Events for Analysis:\n",
      "\n",
      "2017-08-25: Hurricane Harvey (Hurricane)\n",
      " Category 4 hurricane, $125B+ damages, major oil refinery impacts\n",
      "\n",
      "2017-09-10: Hurricane Irma (Hurricane)\n",
      " Category 4 hurricane, Florida impact, insurance claims spike\n",
      "\n",
      "2018-11-08: Camp Fire (Wildfire)\n",
      " Deadliest CA wildfire, PG&E bankruptcy, massive insurance losses\n",
      "\n",
      "2020-01-03: Australia Bushfires (Wildfire)\n",
      " Record-breaking bushfires, global climate concerns\n",
      "\n",
      "2021-02-15: Texas Winter Storm (Extreme Cold)\n",
      " Power grid failure, energy infrastructure collapse\n",
      "\n",
      "2022-09-28: Hurricane Ian (Hurricane)\n",
      " Category 4, $112B+ damages, major insurance event\n",
      "\n",
      "2023-07-18: European Heat Wave (Heat Wave)\n",
      " Record temperatures, infrastructure stress, energy demand\n",
      "\n",
      "\n",
      "Total events: 7\n",
      "Date range: 2017-08-25 to 2023-07-18\n"
     ]
    }
   ],
   "source": [
    "# Major climate events\n",
    "climate_events = {\n",
    "    'Hurricane Harvey': {\n",
    "        'date': '2017-08-25',\n",
    "        'type': 'Hurricane',\n",
    "        'description': 'Category 4 hurricane, $125B+ damages, major oil refinery impacts'},\n",
    "    'Hurricane Irma': {\n",
    "        'date': '2017-09-10', \n",
    "        'type': 'Hurricane',\n",
    "        'description': 'Category 4 hurricane, Florida impact, insurance claims spike'},\n",
    "    'Camp Fire': {\n",
    "        'date': '2018-11-08',\n",
    "        'type': 'Wildfire', \n",
    "        'description': 'Deadliest CA wildfire, PG&E bankruptcy, massive insurance losses'},\n",
    "    'Australia Bushfires': {\n",
    "        'date': '2020-01-03',\n",
    "        'type': 'Wildfire',\n",
    "        'description': 'Record-breaking bushfires, global climate concerns'},\n",
    "    'Texas Winter Storm': {\n",
    "        'date': '2021-02-15',\n",
    "        'type': 'Extreme Cold',\n",
    "        'description': 'Power grid failure, energy infrastructure collapse'},\n",
    "    'Hurricane Ian': {\n",
    "        'date': '2022-09-28',\n",
    "        'type': 'Hurricane', \n",
    "        'description': 'Category 4, $112B+ damages, major insurance event'},\n",
    "    'European Heat Wave': {\n",
    "        'date': '2023-07-18',\n",
    "        'type': 'Heat Wave',\n",
    "        'description': 'Record temperatures, infrastructure stress, energy demand'}\n",
    "}\n",
    "\n",
    "#Converting to DataFrame\n",
    "events_df = pd.DataFrame.from_dict(climate_events, orient='index').reset_index()\n",
    "events_df.columns = ['Event_Name', 'Date', 'Type', 'Description']\n",
    "events_df['Date'] = pd.to_datetime(events_df['Date'])\n",
    "events_df = events_df.sort_values('Date')\n",
    "\n",
    "print(\"Climate Events for Analysis:\")\n",
    "print()\n",
    "for _, event in events_df.iterrows():\n",
    "    print(f\"{event['Date'].strftime('%Y-%m-%d')}: {event['Event_Name']} ({event['Type']})\")\n",
    "    print(f\" {event['Description']}\")\n",
    "    print()\n",
    "print(f\"\\nTotal events: {len(events_df)}\")\n",
    "print(f\"Date range: {events_df['Date'].min().strftime('%Y-%m-%d')} to {events_df['Date'].max().strftime('%Y-%m-%d')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28aba224-63ac-4477-9f1e-d756e9b72ef3",
   "metadata": {},
   "source": [
    "## 4. Data Collection Functions\n",
    "Creating helper functions to download stock data efficiently and handle potential errors, as well as ensure reusablility of code and easy debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d13e2e30-3d7c-4897-adfe-24fb399fa068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data collection functions defined successfully.\n"
     ]
    }
   ],
   "source": [
    "def download_stock_data(ticker, start_date, end_date, max_retries=3):\n",
    "    \"\"\"\n",
    "    Downloading stock data for a single ticker with error handling.\n",
    "\n",
    "    Parameters:\n",
    "    ticker (str): Stock ticker symbol\n",
    "    start_date (str): Start date in 'YYYY-MM-DD' format\n",
    "    end_date (str): End date in 'YYYY-MM-DD' format\n",
    "    max_entries (int): Maximum number of retry attempts\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: Stock data with OHLCV columns\n",
    "    \"\"\"\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try: # Downloading data using yfinance\n",
    "            stock = yf.Ticker(ticker)\n",
    "            data = stock.history(start=start_date, end=end_date)\n",
    "\n",
    "            if data.empty:\n",
    "                print(f\"Warning: No data found for {ticker}\")\n",
    "                return None\n",
    "\n",
    "                # Adding ticker column for identification\n",
    "                data['Ticker'] = ticker\n",
    "                data.reset_index(inplace=True)\n",
    "\n",
    "                # Cleaning column names (removing any extra spaces)\n",
    "                data.columns = data.columns.str.strip()\n",
    "                print(f\"Successfully downloaded {ticker}: {len(data)} records\")\n",
    "                return data\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt + 1} failed for {ticker}: {str(e)}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(2) # To wait 2 seconds before retry\n",
    "            else:\n",
    "                print(f\"Failed to download {ticker} after {max_retries} attempts\")\n",
    "                return None\n",
    "\n",
    "def download_sector_data(tickers, sector_name, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Dowloading stock data for all tickers in a sector.\n",
    "\n",
    "    Parameters:\n",
    "    tickers (list): List of ticker symbols\n",
    "    sector_name (str): Name of the sector for logging\n",
    "    start_date (str): Start date in 'YYYY-MM-DD' format\n",
    "    end_date (str): End date in 'YYYY-MM-DD' format\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: Combined stock data for all tickers\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\nDownloading {sector_name} sector data...\")\n",
    "    print(f\"Tickers: {', '.join(tickers)}\")\n",
    "    print()\n",
    "    \n",
    "    sector_data = []\n",
    "    successful_downloads = 0\n",
    "    for i, ticker in enumerate(tickers, 1):\n",
    "        print(f\"[{i}/{len(tickers)}] Downloading {ticker}...\")\n",
    "        data = download_stock_data(ticker, start_date, end_date)\n",
    "        if data is not None:\n",
    "            data['Sector'] = sector_name\n",
    "            sector_data.append(data)\n",
    "            successful_downloads += 1\n",
    "            \n",
    "        time.sleep(0.5) # Adding small delay (API)\n",
    "        \n",
    "    if sector_data:\n",
    "        combined_data = pd.concat(sector_data, ignore_index=True)\n",
    "        print(f\"\\n{sector_name} sector complete: {successful_downloads}/{len(tickers)} successful downloads\")\n",
    "        print(f\"  Total records: {len(combined_data):,}\")\n",
    "        return combined_data\n",
    "    else:\n",
    "        print(f\"\\nNo data collected for {sector_name} sector\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "def save_data(data, filename, folder='../data/raw/'):\n",
    "    \"\"\"\n",
    "    Saving data to CSV file with error handling.\n",
    "    \n",
    "    Parameters:\n",
    "    data (pandas.DataFrame): Data to save\n",
    "    filename (str): Name of the file (without extension)\n",
    "    folder (str): Folder path to save the file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        os.makedirs(folder, exist_ok=True) # Creating folder if it doesn't exist\n",
    "        filepath = os.path.join(folder, f\"{filename}.csv\") # File path\n",
    "        data.to_csv(filepath, index=False) # Saving to CSV\n",
    "        print(f\"Data saved to: {filepath}\")\n",
    "        print(f\"  File size: {len(data):,} rows * {len(data.columns)} columns\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving {filename}: {str(e)}\")\n",
    "\n",
    "print(\"Data collection functions defined successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f5eeb9-0b8c-4be1-aaa3-972fc3282bd4",
   "metadata": {},
   "source": [
    "## 5. Executing Data Collection\n",
    "Downloading all the stock data (7 years of daily data for 45 companies)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34d8ce22-913c-424f-90ea-29a911700b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data collection for period: 2017-01-01 to 2024-12-31\n",
      "Total companies to download: 45\n",
      "Estimated time: 1.5 minutes\n",
      "\n",
      "\n",
      "Downloading Energy sector data...\n",
      "Tickers: XOM, CVX, COP, EOG, SLB, PXD, KMI, OXY, VLO, PSX, MPC, HAL, BKR, DVN, FANG\n",
      "\n",
      "[1/15] Downloading XOM...\n",
      "[2/15] Downloading CVX...\n",
      "[3/15] Downloading COP...\n",
      "[4/15] Downloading EOG...\n",
      "[5/15] Downloading SLB...\n",
      "[6/15] Downloading PXD...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$PXD: possibly delisted; no timezone found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No data found for PXD\n",
      "[7/15] Downloading KMI...\n",
      "[8/15] Downloading OXY...\n",
      "[9/15] Downloading VLO...\n",
      "[10/15] Downloading PSX...\n",
      "[11/15] Downloading MPC...\n",
      "[12/15] Downloading HAL...\n",
      "[13/15] Downloading BKR...\n",
      "[14/15] Downloading DVN...\n",
      "[15/15] Downloading FANG...\n",
      "\n",
      "No data collected for Energy sector\n",
      "\n",
      "\n",
      "Downloading Insurance sector data...\n",
      "Tickers: BRK-B, PGR, TRV, ALL, CB, AIG, HIG, CNA, RLI, AFG, CINF, WRB, Y, EG, KMPR\n",
      "\n",
      "[1/15] Downloading BRK-B...\n",
      "[2/15] Downloading PGR...\n",
      "[3/15] Downloading TRV...\n",
      "[4/15] Downloading ALL...\n",
      "[5/15] Downloading CB...\n",
      "[6/15] Downloading AIG...\n",
      "[7/15] Downloading HIG...\n",
      "[8/15] Downloading CNA...\n",
      "[9/15] Downloading RLI...\n",
      "[10/15] Downloading AFG...\n",
      "[11/15] Downloading CINF...\n",
      "[12/15] Downloading WRB...\n",
      "[13/15] Downloading Y...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$Y: possibly delisted; no timezone found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No data found for Y\n",
      "[14/15] Downloading EG...\n",
      "[15/15] Downloading KMPR...\n",
      "\n",
      "No data collected for Insurance sector\n",
      "\n",
      "\n",
      "Downloading Real Estate sector data...\n",
      "Tickers: PLD, AMT, CCI, EQIX, WELL, DLR, SPG, O, AVTR, EXR, AVB, EQR, MAA, ESS, UDR\n",
      "\n",
      "[1/15] Downloading PLD...\n",
      "[2/15] Downloading AMT...\n",
      "[3/15] Downloading CCI...\n",
      "[4/15] Downloading EQIX...\n",
      "[5/15] Downloading WELL...\n",
      "[6/15] Downloading DLR...\n",
      "[7/15] Downloading SPG...\n",
      "[8/15] Downloading O...\n",
      "[9/15] Downloading AVTR...\n",
      "[10/15] Downloading EXR...\n",
      "[11/15] Downloading AVB...\n",
      "[12/15] Downloading EQR...\n",
      "[13/15] Downloading MAA...\n",
      "[14/15] Downloading ESS...\n",
      "[15/15] Downloading UDR...\n",
      "\n",
      "No data collected for Real Estate sector\n",
      "\n",
      "\n",
      "No data was successfully collected.\n"
     ]
    }
   ],
   "source": [
    "total_companies = sum(len(tickers) for tickers in sector_tickers.values())\n",
    "start_date = '2017-01-01'\n",
    "end_date = '2024-12-31'\n",
    "\n",
    "print(f\"Starting data collection for period: {start_date} to {end_date}\")\n",
    "print(f\"Total companies to download: {total_companies}\")\n",
    "print(f\"Estimated time: {total_companies * 2 / 60:.1f} minutes\")\n",
    "print()\n",
    "\n",
    "all_sector_data = []\n",
    "collection_start_time = time.time()\n",
    "\n",
    "# Downloading data for each sector\n",
    "for sector, tickers in sector_tickers.items():\n",
    "    sector_data = download_sector_data(tickers, sector, start_date, end_date)\n",
    "    if not sector_data.empty:\n",
    "        all_sector_data.append(sector_data)\n",
    "        save_data(sector_data, f\"{sector.lower().replace(' ', '_')}_stock_data\")\n",
    "    print()\n",
    "\n",
    "# Combining all sectors\n",
    "if all_sector_data:\n",
    "    complete_dataset = pd.concat(all_sector_data, ignore_index=True)\n",
    "    save_data(complete_dataset, \"complete_stock_data\") # Saving combined dataset\n",
    "    collection_end_time = time.time()\n",
    "    total_time = (collection_end_time - collection_start_time) / 60\n",
    "    print(f\"\\nData collection complete.\")\n",
    "    print(f\"Total time: {total_time:.1f} minutes\")\n",
    "    print(f\"Total records collected: {len(complete_dataset):,}\")\n",
    "    print(f\"Date range: {complete_dataset['Date'].min()} to {complete_dataset['Date'].max()}\")\n",
    "    print(f\"Unique companies: {complete_dataset['Ticker'].nunique()}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nNo data was successfully collected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0e4f19f-ac66-4add-bc71-af28106d2221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing single ticker download...\n",
      "Data shape: (2011, 7)\n",
      "Columns: ['Open', 'High', 'Low', 'Close', 'Volume', 'Dividends', 'Stock Splits']\n",
      "First few rows:\n",
      "                                Open       High        Low      Close  \\\n",
      "Date                                                                    \n",
      "2017-01-03 00:00:00-05:00  61.858571  62.130652  61.321202  61.824558   \n",
      "2017-01-04 00:00:00-05:00  61.981001  62.001406  61.049107  61.144337   \n",
      "2017-01-05 00:00:00-05:00  61.348386  61.423210  60.158014  60.232838   \n",
      "2017-01-06 00:00:00-05:00  60.396100  60.525342  59.736292  60.198837   \n",
      "2017-01-09 00:00:00-05:00  60.008393  60.008393  58.872439  59.205742   \n",
      "\n",
      "                             Volume  Dividends  Stock Splits  \n",
      "Date                                                          \n",
      "2017-01-03 00:00:00-05:00  10360600        0.0           0.0  \n",
      "2017-01-04 00:00:00-05:00   9434200        0.0           0.0  \n",
      "2017-01-05 00:00:00-05:00  14443200        0.0           0.0  \n",
      "2017-01-06 00:00:00-05:00  16518100        0.0           0.0  \n",
      "2017-01-09 00:00:00-05:00  13762300        0.0           0.0  \n",
      "Is data empty? False\n"
     ]
    }
   ],
   "source": [
    "# Testing with just one ticker to see what's happening\n",
    "print(\"Testing single ticker download...\")\n",
    "\n",
    "# Trying Exxon Mobil (XOM)\n",
    "test_ticker = \"XOM\"\n",
    "test_data = yf.Ticker(test_ticker)\n",
    "test_result = test_data.history(start='2017-01-01', end='2024-12-31')\n",
    "\n",
    "print(f\"Data shape: {test_result.shape}\")\n",
    "print(f\"Columns: {list(test_result.columns)}\")\n",
    "print(f\"First few rows:\")\n",
    "print(test_result.head())\n",
    "print(f\"Is data empty? {test_result.empty}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95d56489-6706-446f-94c4-08f6774f36d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debugging our download function...\n",
      "Step 1: Creating ticker object for XOM\n",
      "Step 2: Downloading data from 2017-01-01 to 2024-12-31\n",
      "Step 3: Data shape: (2011, 7)\n",
      "Step 4: Data empty? False\n",
      "Step 5: Data columns: ['Open', 'High', 'Low', 'Close', 'Volume', 'Dividends', 'Stock Splits']\n",
      "Step 6: Data index type: <class 'pandas.core.indexes.datetimes.DatetimeIndex'>\n",
      "Step 7: Adding ticker column\n",
      "Step 8: Resetting index\n",
      "Step 9: Final data shape: (2011, 9)\n",
      "Step 10: Final columns: ['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Dividends', 'Stock Splits', 'Ticker']\n",
      "Successfully processed XOM: 2011 records\n",
      "\n",
      "Final result preview:\n",
      "                       Date       Open       High        Low      Close  \\\n",
      "0 2017-01-03 00:00:00-05:00  61.858571  62.130652  61.321202  61.824558   \n",
      "1 2017-01-04 00:00:00-05:00  61.981001  62.001406  61.049107  61.144337   \n",
      "2 2017-01-05 00:00:00-05:00  61.348386  61.423210  60.158014  60.232838   \n",
      "3 2017-01-06 00:00:00-05:00  60.396100  60.525342  59.736292  60.198837   \n",
      "4 2017-01-09 00:00:00-05:00  60.008393  60.008393  58.872439  59.205742   \n",
      "\n",
      "     Volume  Dividends  Stock Splits Ticker  \n",
      "0  10360600        0.0           0.0    XOM  \n",
      "1   9434200        0.0           0.0    XOM  \n",
      "2  14443200        0.0           0.0    XOM  \n",
      "3  16518100        0.0           0.0    XOM  \n",
      "4  13762300        0.0           0.0    XOM  \n"
     ]
    }
   ],
   "source": [
    "# Debugging our function with the working data\n",
    "print(\"Debugging our download function...\")\n",
    "\n",
    "def debug_download_stock_data(ticker, start_date, end_date):\n",
    "    \"\"\"Debug version to see what's happening\"\"\"\n",
    "    \n",
    "    try:\n",
    "        print(f\"Step 1: Creating ticker object for {ticker}\")\n",
    "        stock = yf.Ticker(ticker)\n",
    "        print(f\"Step 2: Downloading data from {start_date} to {end_date}\")\n",
    "        data = stock.history(start=start_date, end=end_date)\n",
    "        print(f\"Step 3: Data shape: {data.shape}\")\n",
    "        print(f\"Step 4: Data empty? {data.empty}\")\n",
    "        print(f\"Step 5: Data columns: {list(data.columns)}\")\n",
    "        print(f\"Step 6: Data index type: {type(data.index)}\")\n",
    "        \n",
    "        if data.empty:\n",
    "            print(f\"Data is empty for {ticker}\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"Step 7: Adding ticker column\")\n",
    "        data['Ticker'] = ticker\n",
    "        print(f\"Step 8: Resetting index\")\n",
    "        data.reset_index(inplace=True)\n",
    "        print(f\"Step 9: Final data shape: {data.shape}\")\n",
    "        print(f\"Step 10: Final columns: {list(data.columns)}\")\n",
    "        print(f\"Successfully processed {ticker}: {len(data)} records\")\n",
    "        return data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {ticker}: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Testing with XOM\n",
    "test_result = debug_download_stock_data(\"XOM\", \"2017-01-01\", \"2024-12-31\")\n",
    "if test_result is not None:\n",
    "    print(f\"\\nFinal result preview:\")\n",
    "    print(test_result.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8731c4c0-8075-4d44-8d5b-0104281b65de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing original function...\n",
      "Successfully downloaded XOM: 2011 records\n",
      "Original function success.\n",
      "Shape: (2011, 9)\n",
      "Columns: ['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Dividends', 'Stock Splits', 'Ticker']\n"
     ]
    }
   ],
   "source": [
    "# Testing original function\n",
    "print(\"Testing original function...\")\n",
    "\n",
    "def download_stock_data(ticker, start_date, end_date, max_retries=3):\n",
    "    \"\"\"\n",
    "    Downloading stock data for a single ticker with error handling.\n",
    "    \"\"\"\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            stock = yf.Ticker(ticker) # Downloading data using yfinance\n",
    "            data = stock.history(start=start_date, end=end_date)\n",
    "            if data.empty:\n",
    "                print(f\"No data found for {ticker}\")\n",
    "                return None\n",
    "            data['Ticker'] = ticker # Adding ticker column for identification\n",
    "            data.reset_index(inplace=True)\n",
    "            data.columns = data.columns.str.strip() # Cleaning column names (removing any extra spaces)\n",
    "            print(f\"Successfully downloaded {ticker}: {len(data)} records\")\n",
    "            return data\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt + 1} failed for {ticker}: {str(e)}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(2)  # Wait 2 seconds before retry\n",
    "            else:\n",
    "                print(f\"Failed to download {ticker} after {max_retries} attempts\")\n",
    "                return None\n",
    "\n",
    "# Testing with XOM using original function\n",
    "original_result = download_stock_data(\"XOM\", \"2017-01-01\", \"2024-12-31\")\n",
    "if original_result is not None:\n",
    "    print(f\"Original function success.\")\n",
    "    print(f\"Shape: {original_result.shape}\")\n",
    "    print(f\"Columns: {list(original_result.columns)}\")\n",
    "else:\n",
    "    print(\"Original function failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "229de61e-473e-456a-98fa-6e69d551d680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing sector function with reliable tickers...\n",
      "\n",
      "Downloading Energy_Test sector data...\n",
      "Tickers: XOM, CVX, COP\n",
      "\n",
      "[1/3] Downloading XOM...\n",
      "Successfully downloaded XOM: 2011 records\n",
      " Added XOM to sector data\n",
      "[2/3] Downloading CVX...\n",
      "Successfully downloaded CVX: 2011 records\n",
      " Added CVX to sector data\n",
      "[3/3] Downloading COP...\n",
      "Successfully downloaded COP: 2011 records\n",
      " Added COP to sector data\n",
      "\n",
      "Energy_Test sector complete: 3/3 successful downloads\n",
      " Total records: 6,033\n",
      "\n",
      "Success. Downloaded 6,033 total records\n",
      "Unique tickers: ['XOM' 'CVX' 'COP']\n"
     ]
    }
   ],
   "source": [
    "# Testing sector function with just a few reliable tickers\n",
    "print(\"Testing sector function with reliable tickers...\")\n",
    "\n",
    "def download_sector_data(tickers, sector_name, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Downloading stock data for all tickers in a sector.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\nDownloading {sector_name} sector data...\")\n",
    "    print(f\"Tickers: {', '.join(tickers)}\")\n",
    "    print()\n",
    "    sector_data = []\n",
    "    successful_downloads = 0\n",
    "    \n",
    "    for i, ticker in enumerate(tickers, 1):\n",
    "        print(f\"[{i}/{len(tickers)}] Downloading {ticker}...\")\n",
    "        data = download_stock_data(ticker, start_date, end_date)\n",
    "        if data is not None:\n",
    "            data['Sector'] = sector_name\n",
    "            sector_data.append(data)\n",
    "            successful_downloads += 1\n",
    "            print(f\" Added {ticker} to sector data\")\n",
    "        else:\n",
    "            print(f\" {ticker} returned None\")\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    if sector_data:\n",
    "        combined_data = pd.concat(sector_data, ignore_index=True)\n",
    "        print(f\"\\n{sector_name} sector complete: {successful_downloads}/{len(tickers)} successful downloads\")\n",
    "        print(f\" Total records: {len(combined_data):,}\")\n",
    "        return combined_data\n",
    "    else:\n",
    "        print(f\"\\nNo data collected for {sector_name} sector\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Testing with just 3 very reliable energy stocks\n",
    "test_energy_tickers = ['XOM', 'CVX', 'COP']  # Exxon, Chevron, ConocoPhillips\n",
    "test_result = download_sector_data(test_energy_tickers, \"Energy_Test\", \"2017-01-01\", \"2024-12-31\")\n",
    "if not test_result.empty:\n",
    "    print(f\"\\nSuccess. Downloaded {len(test_result):,} total records\")\n",
    "    print(f\"Unique tickers: {test_result['Ticker'].unique()}\")\n",
    "else:\n",
    "    print(\"\\nTest failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce816403-f7af-4c9e-82c7-eb55eb37dd11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixing ticker lists...\n",
      "Updated ticker lists with reliable companies\n",
      "Total companies: 45\n"
     ]
    }
   ],
   "source": [
    "# Replacing problematic tickers with reliable ones\n",
    "print(\"Fixing ticker lists...\")\n",
    "\n",
    "# Fixed Energy Sector (replacing PXD with HPE - HP Enterprise, and keep others)\n",
    "energy_tickers = [\n",
    "    'XOM',   # Exxon Mobil\n",
    "    'CVX',   # Chevron\n",
    "    'COP',   # ConocoPhillips\n",
    "    'EOG',   # EOG Resources\n",
    "    'SLB',   # Schlumberger\n",
    "    'WMB',   # Williams Companies (replaced PXD)\n",
    "    'KMI',   # Kinder Morgan\n",
    "    'OXY',   # Occidental Petroleum\n",
    "    'VLO',   # Valero Energy\n",
    "    'PSX',   # Phillips 66\n",
    "    'MPC',   # Marathon Petroleum\n",
    "    'HAL',   # Halliburton\n",
    "    'BKR',   # Baker Hughes\n",
    "    'DVN',   # Devon Energy\n",
    "    'FANG'   # Diamondback Energy\n",
    "]\n",
    "\n",
    "# Fixed Insurance Sector (replacing Y with MET - MetLife)\n",
    "insurance_tickers = [\n",
    "    'BRK-B', # Berkshire Hathaway\n",
    "    'PGR',   # Progressive\n",
    "    'TRV',   # Travelers\n",
    "    'ALL',   # Allstate\n",
    "    'CB',    # Chubb\n",
    "    'AIG',   # American International Group\n",
    "    'HIG',   # Hartford Financial\n",
    "    'CNA',   # CNA Financial\n",
    "    'RLI',   # RLI Corp\n",
    "    'AFG',   # American Financial Group\n",
    "    'CINF',  # Cincinnati Financial\n",
    "    'WRB',   # W.R. Berkley\n",
    "    'MET',   # MetLife (replaced Y)\n",
    "    'EG',    # Everest Group\n",
    "    'KMPR'   # Kemper\n",
    "]\n",
    "\n",
    "# Real Estate - keeping as is\n",
    "real_estate_tickers = [\n",
    "    'PLD',   # Prologis\n",
    "    'AMT',   # American Tower\n",
    "    'CCI',   # Crown Castle\n",
    "    'EQIX',  # Equinix\n",
    "    'WELL',  # Welltower\n",
    "    'DLR',   # Digital Realty Trust\n",
    "    'SPG',   # Simon Property Group\n",
    "    'O',     # Realty Income\n",
    "    'AVB',   # AvalonBay Communities\n",
    "    'EXR',   # Extended Stay America\n",
    "    'EQR',   # Equity Residential\n",
    "    'MAA',   # Mid-America Apartment Communities\n",
    "    'ESS',   # Essex Property Trust\n",
    "    'UDR',   # UDR Inc\n",
    "    'ARE'    # Alexandria Real Estate\n",
    "]\n",
    "\n",
    "# Updating the master dictionary\n",
    "sector_tickers = {\n",
    "    'Energy': energy_tickers,\n",
    "    'Insurance': insurance_tickers,\n",
    "    'Real Estate': real_estate_tickers\n",
    "}\n",
    "print(\"Updated ticker lists with reliable companies\")\n",
    "print(f\"Total companies: {sum(len(tickers) for tickers in sector_tickers.values())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fceaea88-ace2-4735-b6f9-317560c44a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL DATA COLLECTION\n",
      "\n",
      "\n",
      "Starting Energy sector (15 companies)...\n",
      "\n",
      "Downloading Energy sector data...\n",
      "Tickers: XOM, CVX, COP, EOG, SLB, WMB, KMI, OXY, VLO, PSX, MPC, HAL, BKR, DVN, FANG\n",
      "\n",
      "[1/15] Downloading XOM...\n",
      "Successfully downloaded XOM: 2011 records\n",
      " Added XOM to sector data\n",
      "[2/15] Downloading CVX...\n",
      "Successfully downloaded CVX: 2011 records\n",
      " Added CVX to sector data\n",
      "[3/15] Downloading COP...\n",
      "Successfully downloaded COP: 2011 records\n",
      " Added COP to sector data\n",
      "[4/15] Downloading EOG...\n",
      "Successfully downloaded EOG: 2011 records\n",
      " Added EOG to sector data\n",
      "[5/15] Downloading SLB...\n",
      "Successfully downloaded SLB: 2011 records\n",
      " Added SLB to sector data\n",
      "[6/15] Downloading WMB...\n",
      "Successfully downloaded WMB: 2011 records\n",
      " Added WMB to sector data\n",
      "[7/15] Downloading KMI...\n",
      "Successfully downloaded KMI: 2011 records\n",
      " Added KMI to sector data\n",
      "[8/15] Downloading OXY...\n",
      "Successfully downloaded OXY: 2011 records\n",
      " Added OXY to sector data\n",
      "[9/15] Downloading VLO...\n",
      "Successfully downloaded VLO: 2011 records\n",
      " Added VLO to sector data\n",
      "[10/15] Downloading PSX...\n",
      "Successfully downloaded PSX: 2011 records\n",
      " Added PSX to sector data\n",
      "[11/15] Downloading MPC...\n",
      "Successfully downloaded MPC: 2011 records\n",
      " Added MPC to sector data\n",
      "[12/15] Downloading HAL...\n",
      "Successfully downloaded HAL: 2011 records\n",
      " Added HAL to sector data\n",
      "[13/15] Downloading BKR...\n",
      "Successfully downloaded BKR: 2011 records\n",
      " Added BKR to sector data\n",
      "[14/15] Downloading DVN...\n",
      "Successfully downloaded DVN: 2011 records\n",
      " Added DVN to sector data\n",
      "[15/15] Downloading FANG...\n",
      "Successfully downloaded FANG: 2011 records\n",
      " Added FANG to sector data\n",
      "\n",
      "Energy sector complete: 15/15 successful downloads\n",
      " Total records: 30,165\n",
      "Data saved to: ../data/raw/energy_stock_data.csv\n",
      "  File size: 30,165 rows * 10 columns\n",
      " Energy sector saved successfully!\n",
      "\n",
      "Starting Insurance sector (15 companies)...\n",
      "\n",
      "Downloading Insurance sector data...\n",
      "Tickers: BRK-B, PGR, TRV, ALL, CB, AIG, HIG, CNA, RLI, AFG, CINF, WRB, MET, EG, KMPR\n",
      "\n",
      "[1/15] Downloading BRK-B...\n",
      "Successfully downloaded BRK-B: 2011 records\n",
      " Added BRK-B to sector data\n",
      "[2/15] Downloading PGR...\n",
      "Successfully downloaded PGR: 2011 records\n",
      " Added PGR to sector data\n",
      "[3/15] Downloading TRV...\n",
      "Successfully downloaded TRV: 2011 records\n",
      " Added TRV to sector data\n",
      "[4/15] Downloading ALL...\n",
      "Successfully downloaded ALL: 2011 records\n",
      " Added ALL to sector data\n",
      "[5/15] Downloading CB...\n",
      "Successfully downloaded CB: 2011 records\n",
      " Added CB to sector data\n",
      "[6/15] Downloading AIG...\n",
      "Successfully downloaded AIG: 2011 records\n",
      " Added AIG to sector data\n",
      "[7/15] Downloading HIG...\n",
      "Successfully downloaded HIG: 2011 records\n",
      " Added HIG to sector data\n",
      "[8/15] Downloading CNA...\n",
      "Successfully downloaded CNA: 2011 records\n",
      " Added CNA to sector data\n",
      "[9/15] Downloading RLI...\n",
      "Successfully downloaded RLI: 2011 records\n",
      " Added RLI to sector data\n",
      "[10/15] Downloading AFG...\n",
      "Successfully downloaded AFG: 2011 records\n",
      " Added AFG to sector data\n",
      "[11/15] Downloading CINF...\n",
      "Successfully downloaded CINF: 2011 records\n",
      " Added CINF to sector data\n",
      "[12/15] Downloading WRB...\n",
      "Successfully downloaded WRB: 2011 records\n",
      " Added WRB to sector data\n",
      "[13/15] Downloading MET...\n",
      "Successfully downloaded MET: 2011 records\n",
      " Added MET to sector data\n",
      "[14/15] Downloading EG...\n",
      "Successfully downloaded EG: 2011 records\n",
      " Added EG to sector data\n",
      "[15/15] Downloading KMPR...\n",
      "Successfully downloaded KMPR: 2011 records\n",
      " Added KMPR to sector data\n",
      "\n",
      "Insurance sector complete: 15/15 successful downloads\n",
      " Total records: 30,165\n",
      "Data saved to: ../data/raw/insurance_stock_data.csv\n",
      "  File size: 30,165 rows * 10 columns\n",
      " Insurance sector saved successfully!\n",
      "\n",
      "Starting Real Estate sector (15 companies)...\n",
      "\n",
      "Downloading Real Estate sector data...\n",
      "Tickers: PLD, AMT, CCI, EQIX, WELL, DLR, SPG, O, AVB, EXR, EQR, MAA, ESS, UDR, ARE\n",
      "\n",
      "[1/15] Downloading PLD...\n",
      "Successfully downloaded PLD: 2011 records\n",
      " Added PLD to sector data\n",
      "[2/15] Downloading AMT...\n",
      "Successfully downloaded AMT: 2011 records\n",
      " Added AMT to sector data\n",
      "[3/15] Downloading CCI...\n",
      "Successfully downloaded CCI: 2011 records\n",
      " Added CCI to sector data\n",
      "[4/15] Downloading EQIX...\n",
      "Successfully downloaded EQIX: 2011 records\n",
      " Added EQIX to sector data\n",
      "[5/15] Downloading WELL...\n",
      "Successfully downloaded WELL: 2011 records\n",
      " Added WELL to sector data\n",
      "[6/15] Downloading DLR...\n",
      "Successfully downloaded DLR: 2011 records\n",
      " Added DLR to sector data\n",
      "[7/15] Downloading SPG...\n",
      "Successfully downloaded SPG: 2011 records\n",
      " Added SPG to sector data\n",
      "[8/15] Downloading O...\n",
      "Successfully downloaded O: 2011 records\n",
      " Added O to sector data\n",
      "[9/15] Downloading AVB...\n",
      "Successfully downloaded AVB: 2011 records\n",
      " Added AVB to sector data\n",
      "[10/15] Downloading EXR...\n",
      "Successfully downloaded EXR: 2011 records\n",
      " Added EXR to sector data\n",
      "[11/15] Downloading EQR...\n",
      "Successfully downloaded EQR: 2011 records\n",
      " Added EQR to sector data\n",
      "[12/15] Downloading MAA...\n",
      "Successfully downloaded MAA: 2011 records\n",
      " Added MAA to sector data\n",
      "[13/15] Downloading ESS...\n",
      "Successfully downloaded ESS: 2011 records\n",
      " Added ESS to sector data\n",
      "[14/15] Downloading UDR...\n",
      "Successfully downloaded UDR: 2011 records\n",
      " Added UDR to sector data\n",
      "[15/15] Downloading ARE...\n",
      "Successfully downloaded ARE: 2011 records\n",
      " Added ARE to sector data\n",
      "\n",
      "Real Estate sector complete: 15/15 successful downloads\n",
      " Total records: 30,165\n",
      "Data saved to: ../data/raw/real_estate_stock_data.csv\n",
      "  File size: 30,165 rows * 10 columns\n",
      " Real Estate sector saved successfully!\n",
      "Data saved to: ../data/raw/complete_stock_data.csv\n",
      "  File size: 90,495 rows * 10 columns\n",
      "Data saved to: ../data/raw/climate_events.csv\n",
      "  File size: 7 rows * 4 columns\n",
      "\n",
      "Complete success.\n",
      " Total time: 0.4 minutes\n",
      " Total records: 90,495\n",
      " Companies: 45\n",
      " Date range: 2017-01-03 00:00:00-05:00 to 2024-12-30 00:00:00-05:00\n",
      " Sectors: Energy, Insurance, Real Estate\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Final data collection (all sectors)\n",
    "print(\"FINAL DATA COLLECTION\")\n",
    "print()\n",
    "\n",
    "start_date = '2017-01-01'\n",
    "end_date = '2024-12-31'\n",
    "all_sector_data = []\n",
    "collection_start_time = time.time()\n",
    "\n",
    "# Downloading all sectors\n",
    "for sector, tickers in sector_tickers.items():\n",
    "    print(f\"\\nStarting {sector} sector ({len(tickers)} companies)...\")\n",
    "    sector_data = download_sector_data(tickers, sector, start_date, end_date)\n",
    "    if not sector_data.empty:\n",
    "        all_sector_data.append(sector_data)\n",
    "        save_data(sector_data, f\"{sector.lower().replace(' ', '_')}_stock_data\")\n",
    "        print(f\" {sector} sector saved successfully!\")\n",
    "\n",
    "# Combining and saving everything\n",
    "if all_sector_data:\n",
    "    complete_dataset = pd.concat(all_sector_data, ignore_index=True)\n",
    "    save_data(complete_dataset, \"complete_stock_data\")\n",
    "    save_data(events_df, \"climate_events\") # Saving climate events too\n",
    "    total_time = (time.time() - collection_start_time) / 60\n",
    "    print()\n",
    "    print(\"Complete success.\")\n",
    "    print(f\" Total time: {total_time:.1f} minutes\")\n",
    "    print(f\" Total records: {len(complete_dataset):,}\")\n",
    "    print(f\" Companies: {complete_dataset['Ticker'].nunique()}\")\n",
    "    print(f\" Date range: {complete_dataset['Date'].min()} to {complete_dataset['Date'].max()}\")\n",
    "    print(f\" Sectors: {', '.join(complete_dataset['Sector'].unique())}\")\n",
    "    print()\n",
    "else:\n",
    "    print(\"Collection failed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f616c2b-361c-4e72-8ead-7633321b1dfe",
   "metadata": {},
   "source": [
    "## 6. Data Quality Check\n",
    "Examining collected data to ensure quality and identify any potential issues before analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd6557c7-9fab-4b19-b6b6-1ac1297ca7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Overview\n",
      "\n",
      "- Shape: (90495, 10)\n",
      "- Memory usage: 15.0 MB\n",
      "- Columns: ['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Dividends', 'Stock Splits', 'Ticker', 'Sector']\n",
      "\n",
      "\n",
      "Missing Data Check\n",
      "\n",
      "No missing data found.\n",
      "\n",
      "\n",
      "Date Range Verification\n",
      "\n",
      "- First date: 2017-01-03 00:00:00-05:00\n",
      "- Last date: 2024-12-30 00:00:00-05:00\n",
      "- Total trading days: 2,011\n",
      "- Time span: 2918 days\n",
      "\n",
      "\n",
      "Sector Distribution\n",
      "\n",
      "             Companies  Records  Avg_Price  Price_StdDev\n",
      "Sector                                                  \n",
      "Energy              15    30165      58.28         38.54\n",
      "Insurance           15    30165     101.28         82.09\n",
      "Real Estate         15    30165     141.83        138.19\n",
      "\n",
      "\n",
      "Company Verification\n",
      "\n",
      "Energy: 15 companies\n",
      " Tickers: BKR, COP, CVX, DVN, EOG, FANG, HAL, KMI, MPC, OXY, PSX, SLB, VLO, WMB, XOM\n",
      "\n",
      "Insurance: 15 companies\n",
      " Tickers: AFG, AIG, ALL, BRK-B, CB, CINF, CNA, EG, HIG, KMPR, MET, PGR, RLI, TRV, WRB\n",
      "\n",
      "Real Estate: 15 companies\n",
      " Tickers: AMT, ARE, AVB, CCI, DLR, EQIX, EQR, ESS, EXR, MAA, O, PLD, SPG, UDR, WELL\n",
      "\n",
      "\n",
      "Price Range Analysis\n",
      "\n",
      "Price Summary by Sector:\n",
      "               min     max    mean\n",
      "Sector                            \n",
      "Energy        4.03  213.36   58.28\n",
      "Insurance    15.58  483.08  101.28\n",
      "Real Estate  24.38  974.66  141.83\n",
      "\n",
      "\n",
      "No extreme price values detected\n",
      "\n",
      "\n",
      "Volume Analysis\n",
      "\n",
      "Average Daily Volume by Sector:\n",
      "                  mean     median\n",
      "Sector                           \n",
      "Energy       8452624.0  6594700.0\n",
      "Insurance    1906187.0  1307475.0\n",
      "Real Estate  1679829.0  1361000.0\n",
      "\n",
      "\n",
      "Data Quality Summary\n",
      "\n",
      "- Total records: 90,495\n",
      "- Companies: 45\n",
      "- Sectors: 3\n",
      "- Date range: 2,011 trading days\n",
      "- Data completeness: 100.0%\n",
      "Dataset ready for analysis.\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset Overview\")\n",
    "print()\n",
    "print(f\"- Shape: {complete_dataset.shape}\")\n",
    "print(f\"- Memory usage: {complete_dataset.memory_usage(deep=True).sum() / 1024 / 1024:.1f} MB\")\n",
    "print(f\"- Columns: {list(complete_dataset.columns)}\")\n",
    "print()\n",
    "\n",
    "print(\"\\nMissing Data Check\")\n",
    "print()\n",
    "missing_data = complete_dataset.isnull().sum()\n",
    "if missing_data.sum() > 0:\n",
    "    print(\"Missing data found:\")\n",
    "    print(missing_data[missing_data > 0])\n",
    "else:\n",
    "    print(\"No missing data found.\")\n",
    "print()\n",
    "\n",
    "print(\"\\nDate Range Verification\")\n",
    "print()\n",
    "complete_dataset['Date'] = pd.to_datetime(complete_dataset['Date'])\n",
    "print(f\"- First date: {complete_dataset['Date'].min()}\")\n",
    "print(f\"- Last date: {complete_dataset['Date'].max()}\")\n",
    "print(f\"- Total trading days: {complete_dataset['Date'].nunique():,}\")\n",
    "print(f\"- Time span: {(complete_dataset['Date'].max() - complete_dataset['Date'].min()).days} days\")\n",
    "print()\n",
    "\n",
    "print(\"\\nSector Distribution\")\n",
    "print()\n",
    "sector_stats = complete_dataset.groupby('Sector').agg({\n",
    "    'Ticker': 'nunique',\n",
    "    'Close': ['count', 'mean', 'std']\n",
    "}).round (2)\n",
    "sector_stats.columns = ['Companies', 'Records', 'Avg_Price', 'Price_StdDev']\n",
    "print(sector_stats)\n",
    "print()\n",
    "\n",
    "print(\"\\nCompany Verification\")\n",
    "print()\n",
    "companies_per_sector = complete_dataset.groupby('Sector')['Ticker'].unique()\n",
    "for sector, tickers in companies_per_sector.items():\n",
    "    print(f\"{sector}: {len(tickers)} companies\")\n",
    "    print(f\" Tickers: {', '.join(sorted(tickers))}\")\n",
    "    print()\n",
    "\n",
    "print(\"\\nPrice Range Analysis\")\n",
    "print()\n",
    "price_summary = complete_dataset.groupby('Sector')['Close'].agg(['min', 'max', 'mean']).round(2)\n",
    "print(\"Price Summary by Sector:\")\n",
    "print(price_summary)\n",
    "print()\n",
    "\n",
    "# Checking extreme values or potential data errors\n",
    "extreme_prices = complete_dataset[(complete_dataset['Close'] < 1) | (complete_dataset['Close'] > 1000)]\n",
    "if len(extreme_prices) > 0:\n",
    "    print(f\"\\nFound {len(extreme_prices)} records with extreme prices (< $1 or > $1000):\")\n",
    "    print(extreme_prices[['Date', 'Ticker', 'Sector', 'Close']].head())\n",
    "else:\n",
    "    print(\"\\nNo extreme price values detected\")\n",
    "print()\n",
    "\n",
    "print(\"\\nVolume Analysis\")\n",
    "print()\n",
    "volume_stats = complete_dataset.groupby('Sector')['Volume'].agg(['mean', 'median']).round(0)\n",
    "print(\"Average Daily Volume by Sector:\")\n",
    "print(volume_stats)\n",
    "print()\n",
    "print()\n",
    "print(\"Data Quality Summary\")\n",
    "print()\n",
    "print(f\"- Total records: {len(complete_dataset):,}\")\n",
    "print(f\"- Companies: {complete_dataset['Ticker'].nunique()}\")\n",
    "print(f\"- Sectors: {complete_dataset['Sector'].nunique()}\")\n",
    "print(f\"- Date range: {complete_dataset['Date'].nunique():,} trading days\")\n",
    "print(f\"- Data completeness: {(1 - complete_dataset.isnull().sum().sum() / complete_dataset.size) * 100:.1f}%\")\n",
    "print(\"Dataset ready for analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62f46b6-e9ce-4984-a840-94503cf0bf4a",
   "metadata": {},
   "source": [
    "## 7. Collection Summary\n",
    "### Tasks Completed:\n",
    "- Successfully collected **90,495 stock price records** from **45 companies** across 3 climate-sensitive sectors\n",
    "- **100% data completeness** with no mission values\n",
    "- **8 years of daily data** (2017-2024) covering 2,011 trading days\n",
    "- **7 major climate events** identified for analysis\n",
    "- All data saved in organized CSV format for subsequent analysis\n",
    "\n",
    "### Data Files Created:\n",
    "- `complete_stock_data.csv`: Master dataset (90,495 records)\n",
    "- `energy_stock_data.csv`: Energy sector (30,165 records)\n",
    "- `insurance_stock_data.csv`: Insurance sector (30,165 records) \n",
    "- `real_estate_stock_data.csv`: Real Estate sector (30,165 records)\n",
    "- `climate_events.csv`: Climate events for analysis (7 events)\n",
    "\n",
    "**Dataset ready for climate risk premium analysis.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
